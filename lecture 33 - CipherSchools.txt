lecture 33 - Introduction To Nlp and Chatbot Frameworks


Common NLP Tasks

Tokenization is the process of breaking text into individual words or tokens.

Stemming and Lemmatization
Stemming and lemmatization are processes of reducing words to their root forms.

Stop Words Removal
Stop words are common words that add little meaning to text and are often removed in preprocessing.

import nltk
nitk.download( 'punkt' )
from nltk.tokenize import word_tokenize
text= "Natural Language Processing is fascinating."
tokens = word_tokenize(text)
print (tokens)


from nltk.stem import PorterStemmer

stemmer=PorterStemmer()
words=["running","run","runs"]
stems=[stemmer.stem(word) for word in words]
print(stems)


from nltk. stem import WordNetLemmatizer
nltk.download( 'wordnet')

lemmatizer = WordNetLemmatizer()
words =["running", "ran", "runs"]
lemmas= [lemmatizer.lemmatize(word, pos='v') for word in words]
print ( lemmas )


from nltk.corpus import stopwords
nltk.download( 'stopwords' )

stop_words = set(stopwords.words('english'))
filtered_text = [word for word in tokens if word.lower() not in stop_words]
print(filtered_text)



